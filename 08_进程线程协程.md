Q : Python中的GIL是什么？

A : 首先GIL并不是Python的特性，而是解释器CPython引入的概念。中文名是全局解释器锁，类似于操作系统的Mutex。GIL的功能是在CPython解释器中执行的每一个Python线程，都先锁住自己，不让别的线程执行。所以实际上这些线程是伪并行。



Q : 为什么要引入GIL呢？

A : 虽然性能并没有提高，但是实现了模拟多线程。而GIL存在的原因是CPython使用了引用计数器来管理内存。

- 假设有两个 Python 线程同时引用 a，那么双方就都会尝试操作该数据，很有可能造成引用计数的条件竞争，导致引用计数只增加 1（实际应增加 2），这造成的后果是，当第一个线程结束时，会把引用计数减少 1，此时可能已经达到释放内存的条件（引用计数为 0），当第 2 个线程再次视图访问 a 时，就无法找到有效的内存了。
- 所以，CPython 引入 GIL，可以最大程度上规避类似内存管理这样复杂的竞争风险问题。

```python
# 引用计数器例子
import sys

a = [1]
b = a
print(sys.getrefcount(a))
```

- 这个例子中，因为a、b、c、getrefcount都有引用[1]这个列表，所以是四次。



Q : 那么有GIL锁了是否线程就是一定安全了？

A : 虽然已经有了GIL，但还是要注意线程安全。因为check_interval机制，也是会导致出现线程安全问题的。

```python
import threading

num = 0

def change_num(n):
    global num
    num += n
    num -= n

def run(n):
    for _ in range(1000):
        change_num(n)

def main():
    thread1 = threading.Thread(target=run,args=[5])
    thread2 = threading.Thread(target=run,args=[6])
    thread1.start()
    thread2.start()
    thread1.join()
    thread2.join()

main()
print(num)
```

- 发现每次的结果都不一样，是因为num+=1实际上是由4个bytecode组成的，GIL并没有注重这个问题。所以在使用python的多线程时依然需要lock()进行加锁。



Q : Python中如何使用多线程的基本操作？

A : 

- 使用threading模块的一些常用方法
- run()线程活动的方法
- Start()启动线程
- join()等待到线程中止
- isAlive/getName/setName

```python
import threading
import time

def aaa(word):
    for _ in range(3):
        print('aaa and %s'%word)
        time.sleep(1)


def bbb(word):
    for _ in range(3):
        print('bbb and %s'%word)
        time.sleep(1)

def main():
    threads = []
    thread1 = threading.Thread(target=aaa,args=['aaa'])
    thread2 = threading.Thread(target=bbb,args=['bbb'])
    threads.append(thread1)
    threads.append(thread2)
    for t in threads:
        t.start()
    for t in threads:
        t.join()

main()
```





Q : 多个线程一起对某个数据修改，可能会出现不可预料的结果，为了确保数据的正确性，需要多个进程同步的。如何使用threading中的Lock进行进程同步。

A : 

```python
# threading.Lock()
# acquire和release获取和释放锁
# coding:utf-8
import threading
import time

class MyThread(threading.Thread):
    def __init__(self,threadID,name,counter):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.counter = counter

    def run(self):
        print("开启进程：" + self.name)
        # 获取
        threadLock.acquire()
        print_time(self.name, self.counter, 3)
        threadLock.release()

def print_time(threadName, delay, counter):
    while counter:
        time.sleep(delay)
        print ("%s: %s" % (threadName, time.ctime(time.time())))
        counter -= 1


threadLock = threading.Lock()
threads = []


# 创建进程实例
thread1 = MyThread(1, 'Thread-1', 1)
thread2 = MyThread(2, 'Thread-2', 2)

thread1.start()
thread2.start()

threads.append(thread1)
threads.append(thread2)

for t in threads:
    t.join()
print('done')
>>>
开启进程：Thread-1
开启进程：Thread-2
Thread-1: Sat Jul 31 22:15:26 2021
Thread-1: Sat Jul 31 22:15:27 2021
Thread-1: Sat Jul 31 22:15:28 2021
Thread-2: Sat Jul 31 22:15:30 2021
Thread-2: Sat Jul 31 22:15:32 2021
Thread-2: Sat Jul 31 22:15:34 2021
done
```



Q : 如何使用线程优先级队列来实现进程间的同步？

A : Python的Queue模块中就提供了同步且线程安全的队列类。

一些常用方法：

- Queue.empty():队列是否空
- Queue.full():队列是否满
- Queue.get():获取队列
- Queue.put():写入队列
- Queue.task_done():完成一个task后，向队列发出完成信号
- Queue.join():等待队列为空

```python
# coding:utf-8
import threading
import queue
import time

exitFlag = 0

class myThread(threading.Thread):
    def __init__(self, threadID, name, q):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.q = q
    
    def run(self):
        print('开启线程：' + self.name)
        process_data(self.name, self.q)
        print ("退出线程：" + self.name)


def process_data(threadName, q):
    while not exitFlag:
        queueLock.acquire()
        if not workQueue.empty():
            data = q.get()
            queueLock.release()
            print ("%s processing %s" % (threadName, data))
        else:
            queueLock.release()
        time.sleep(1)

threadList = ["Thread-1", "Thread-2", "Thread-3"]
nameList = ["One", "Two", "Three", "Four", "Five"]
queueLock = threading.Lock()
workQueue = queue.Queue(10)
threads = []
threadID = 1

# 创建线程
for tName in threadList:
    thread = myThread(threadID, tName, workQueue)
    thread.start()
    threads.append(thread)
    threadID += 1

# 填充队列
queueLock.acquire()
for word in nameList:
    workQueue.put(word)
queueLock.release()

# 等待队列任务被消化掉
while not workQueue.empty():
    pass

exitFlag = 1
for t in threads:
    t.join()
print('exit main thread')
>>>
开启线程：Thread-1
开启线程：Thread-2
开启线程：Thread-3
Thread-3 processing One
Thread-1 processing Two
Thread-2 processing Three
Thread-3 processing Four
Thread-1 processing Five
退出线程：Thread-3
退出线程：Thread-1
退出线程：Thread-2
exit main thread
```





Q : Python的多进程和多线程有什么区别？

A : python中的多线程并不是真正意义上的多线程，想要充分利用cpu的多核资源，大部分时候情况使用多进程。python中的multiprocessing可以轻松的完成单进程到并发执行的转换。提供了Process、Queue、Pipe、Lock等组件。

```python
from multiprocessing import Process
import os

def info(title):
    print(title)
    print('module name:',__name__)
    print('parent process:', os.getppid())
    print('process id:', os.getpid())

def f(name):
    info('function f')
    print('hello',name)

if __name__ == '__main__':
    info('main line')
    p = Process(target=f, args=('bob',))
    p.start()
    p.join()

>>>
main line
('module name:', '__main__')
('parent process:', 12121)
('process id:', 13848)
function f
('module name:', '__main__')
('parent process:', 13848)
('process id:', 13849)
('hello', 'bob')
```



Q : multiprocessing有多种启动进程的方法，分别有什么不同？

A : 有spawn、fork、forkserver三种启动方法

- spawn：父进程会启动一个全新的python解释器进程，子进程只继承了运行对象的run方法所需的资源。比较慢。
- fork：父进程使用os.fork()来产生python解释器分叉。
- forkserver：启动服务器进程，每当需要一个新进程时，父进程就会链接到服务器并请求它分叉一个新进程。



Q : 多进程中的start和join分别有什么作用？

A : start会给进程进行启动。而join会阻塞主进程直到当前子进程完成为止。



Q : 在使用Process时候使用函数和类的区别？

A : 

```python
# 函数方式
import multiprocessing
import time

def work_1(interval):
    print("work-1")
    time.sleep(interval)
    print("End work-1")

def work_2(interval):
    print("work-2")
    time.sleep(interval)
    print("End work-2")

def work_3(interval):
    print("work-3")
    time.sleep(interval)
    print("End work-3")

if __name__ == '__main__':
    p1 = multiprocessing.Process(target = work_1, args = (2,))
    p2 = multiprocessing.Process(target = work_2, args = (3,))
    p3 = multiprocessing.Process(target = work_3, args = (4,))

    p1.start()
    p2.start()
    p3.start()
    

    print("The number of CPU is:" + str(multiprocessing.cpu_count()))
    for p in multiprocessing.active_children():
        print("child p.name:" + p.name + "\tpid:" + str(p.pid))
    
    print("END")

    p1.join()
    p2.join()
    p2.join()
>>>
work-1
work-2
work-3
The number of CPU is:4
child p.name:Process-3	pid:14503
child p.name:Process-1	pid:14501
child p.name:Process-2	pid:14502
END
End work-1
End work-2
End work-3
```

```python
# Process使用类
import multiprocessing
import time

class ClockProcess(multiprocessing.Process):
    def __init__(self,interval):
        multiprocessing.Process.__init__(self)
        self.interval = interval

    
    def run(self):
        n = 5
        while n > 0:
            print("The time is {0}".format(time.ctime()))
            time.sleep(self.interval)
            n -= 1
    
if __name__ == '__main__':
    p = ClockProcess(3)
    p.start()
>>>
The time is Mon Aug  2 17:38:06 2021
The time is Mon Aug  2 17:38:09 2021
The time is Mon Aug  2 17:38:12 2021
The time is Mon Aug  2 17:38:15 2021
The time is Mon Aug  2 17:38:18 2021
```



Q : 如何在多进程中使用Lock进行同步或者不使用锁的情况下进行同步操作？

A : 

```python
# 在不使用锁的情况下进行同步
import multiprocessing
import time

def job(v, num):
    for _ in range(5):
        time.sleep(0.1)
        v.value += num
        print(v.value, end=",")

def multicore():
    # 定义共享变量
    v = multiprocessing.Value('i', 0)
    p1 = multiprocessing.Process(target=job,args=(v, 1))
    p2 = multiprocessing.Process(target=job,args=(v, 3))

    p1.start()
    p2.start()


if __name__ == '__main__':
    multicore()
# 发现会争夺共享资源
>>>
1,5,9,13,17,4,8,12,16,20
```

```python
import multiprocessing
import time
# 使用锁进行同步 使得资源不会抢占
lock = multiprocessing.Lock()

def job(v, num, lock):
    lock.acquire()
    for _ in range(5):
        time.sleep(0.1)
        v.value += num
        print(v.value, end=",")
    lock.release()

def multicore():
    # 定义共享变量
    v = multiprocessing.Value('i', 0)
    p1 = multiprocessing.Process(target=job,args=(v, 1, lock))
    p2 = multiprocessing.Process(target=job,args=(v, 3, lock))

    p1.start()
    p2.start()


if __name__ == '__main__':
    multicore()
>>>
3,6,9,12,15,16,17,18,19,20
```



Q : Python的进程池如何使用，是同步还是异步的？要如何实现异步？

A : Multiprocessing.Pool可以提供指定数量的进程供用户调用，当新的请求提交到pool中，如果池子没满，那么就会创建一个新的进程来执行请求。如果池子的请求数已经达到最大值，那么请求就会等待，直到池中有进程结束，才会创建新进程来执行他。

```python
import multiprocessing
import time

def test(interval):
    print(interval)
    time.sleep(3)


if __name__ == '__main__':
    pool = multiprocessing.Pool(processes=10)
    for i in range(10):
        pool.apply(test, args=(i,))
    
    print('test')
    pool.close()
    pool.join()
>>>
0
1
2
3
....
# 发现这样进程会阻塞 一个个子进程执行 最后回到被join阻塞的主进程。
```

```python
# 改用apply_async实现异步进程池
import multiprocessing
import time

def test(interval):
    print(interval)
    time.sleep(3)


if __name__ == '__main__':
    pool = multiprocessing.Pool(processes=10)
    for i in range(10):
        pool.apply_async(test, args=(i,))
    
    print('test')
    pool.close()
    pool.join()
>>>
test
0
1
2
...
# 程序在异步执行，没有阻塞。
```

```python
	# 同样还有map_async可以进行map的异步操作
  import multiprocessing
import time

def test(interval):
    print(interval)
    time.sleep(3)


if __name__ == '__main__':
    pool = multiprocessing.Pool(processes=2)
    pool.map_async(test,range(500))
    print('test')
    pool.close()
    pool.join()
>>>
test
0
64
63
234
---
```



Q : 多进程中如何进行不同进程的沟通交流？

A : 主要有Pipe和Queue两种方式。其中Pipe就是管道模式，只适用于两个进程一读一写的单双工，也就是说信息只向一个方向流动。Pipe的读写效率比Queue要高。

当主进程创建Pipe的时候，Pipe的两个Connections连接的的都是主进程。
当主进程创建子进程后，Connections也被拷贝了一份。此时有了4个Connections。
此后，关闭主进程的一个Out Connection，关闭一个子进程的一个In Connection。那么就建立好了一个输入在主进程，输出在子进程的管道。

```python
# coding:utf-8
from multiprocessing import Process, Pipe

def son_process(x, pipe):
    _out_pipe, _in_pipe = pipe

    # 关闭fork过来的输入端
    _in_pipe.close()
    while True:
        try:
            msg = _out_pipe.recv()
            print(msg)
        except EOFError:
            # 当out_pipe接受不到输出的时候且输入被关闭的时候，会抛出EORFError，可以捕获并且退出子进程
            break


if __name__ == '__main__':
    out_pipe, in_pipe = Pipe(True)
    son_p = Process(target=son_process, args=(100, (out_pipe, in_pipe)))
    son_p.start()

    # 等 pipe 被 fork 后，关闭主进程的输出端
    # 这样，创建的Pipe一端连接着主进程的输入，一端连接着子进程的输出口
    # 当pipe的输入端被关闭，且无法接收到输入的值，那么就会抛出EOFError。
    out_pipe.close()
    for i in range(1000):
        in_pipe.send(i)
    in_pipe.close()
    son_p.join()
    print('Main Done')
```

Queue则是使用get和put 方法，可以在多个process中进行。

```python
# coding:utf-8

from multiprocessing import Queue, Process
from queue import Empty as QueueEmpty
import random

def getter(name, queue):
    print('Son process %s') % name
    while True:
        try:
            value = queue.get(True, 10)
            # block为True,就是如果队列中无数据了。
            #   |—————— 若timeout默认是None，那么会一直等待下去。
            #   |—————— 若timeout设置了时间，那么会等待timeout秒后才会抛出Queue.Empty异常
            # block 为False，如果队列中无数据，就抛出Queue.Empty异常
            print("Process getter get: %f") % value
        except QueueEmpty:
            break


def putter(name, queue):
    print("Son process %s") % name
    for i in range(0, 1000):
        value = random.random()
        queue.put(value)
        # 放入数据 put(obj[, block[, timeout]])
        # 若block为True，如队列是满的：
        #  |—————— 若timeout是默认None，那么就会一直等下去
        #  |—————— 若timeout设置了等待时间，那么会等待timeout秒后，如果还是满的，那么就抛出Queue.Full.
        # 若block是False，如果队列满了，直接抛出Queue.Full
        print("Process putter put: %f") % value

if __name__ == '__main__':
    queue = Queue()
    getter_process = Process(target=getter, args=("Getter", queue))
    putter_process = Process(target=putter, args=("Putter", queue))
    getter_process.start()
    putter_process.start()
```



### 协程

- 通常认为线程是轻量级的进程，协程可以理解成微线程
- 通常python中进行并发变成都是使用多进程或者多线程，计算型任务由于GIL的存在所以我们多使用多进程，IO密集的任务，可以通过在IO时让出GIL，使用多线程，达到表面上的并发。其实对于IO型任务我们还有一种选择就是协程，协程是运行在单线程当中的"并发"，协程相比多线程一大优势就是省去了多线程之间的切换开销，获得了更大的运行效率。
- 协程的作用：
  - 在执行函数A时可以随时中断去执行函数B，然后中断函数B继续执行函数A。但这个过程不是函数调用，这个过程像多线程，其实协程只有一个线程执行。
- 协程的优势
  - 执行效率极高，不是线程切换，而是程序自身控制，没有线程开销。
  - 不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在控制共享资源时也不需要加锁，因此执行效率高很多。



### asyncio + yield from

- asyncio在Python3.4引入的标准库，直接内置了对异步IO的支持。

```python
import asyncio
@asyncio.coroutine
def test(i):
    print('test_1', i)
    r = yield from asyncio.sleep(1)
    print('test_2', i)

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    tasks = [test(i) for i in range(3)]
    loop.run_until_complete(asyncio.wait(tasks))
    loop.close()
```

- @asyncio.coroutine把一个generator标记为coroutine类型，然后就把这个coroutine扔到EventLoop中执行。test()会先打印test_1,然后yield from语法可以让我们方便地调用另一个generator。线程不会等待asyncio.sleep，而是直接中断并执行下一个消息循环。当其返回时，线程就可以从yield from 拿到返回值，然后接着执行下一行。把asyncio.sleep看成一个耗时1秒的IO操作，祝线程未等待，而是去执行EventLoop中其他可以执行的coroutine了，因此可以实现并发。



### asyncio + async/await

- Python3.5后引入async/await,让coroutine更加简洁易读。

  - @asyncio.coroutine换成async
  - 把yield from换成await

  ```python
  import asyncio
  async def test(i):
      print('test_1',i)
      await asyncio.sleep(1)
      print('test_2')
  
  if __name__ == '__main__':
      loop = asyncio.get_event_loop()
      tasks = [test(i) for i in range(3)]
      loop.run_until_complete(asyncio.wait(tasks))
      loop.close()
  ```



### Gevent

- 是一个基于Greenlet实现的网络库，通过greenlet实现协程。基于一个greenlet就认为是一个协程，当一个greenlet遇到IO操作，就会自动切换到其他greenlet，等到IO操作完成，再切回来。而gevent帮我们自动切换，保证总有greenlet在运行，不去等待IO。

```python
# coding:utf-8
import gevent

def test(n):
    for i in range(n):
        print(gevent.getcurrent(), i)
        # 模拟交出控制权
        gevent.sleep(1)

if __name__ == '__main__':
    gevent.joinall([
        gevent.spawn(test,3),
        gevent.spawn(test,3),
        gevent.spawn(test,3)
    ])
```

